#!/usr/bin/env python3
"""
Script de Teste para OLABOT v2 - Sistema de Engenharia de Prompt
===============================================================

Este script testa todas as funcionalidades do novo sistema OLABOT
incluindo engenharia de prompt, detecÃ§Ã£o de contexto e otimizaÃ§Ã£o de respostas.

Uso:
    python test_olabot.py
"""

import os
import sys
import time
from typing import Dict, List

# Adicionar diretÃ³rio pai ao path para importaÃ§Ãµes
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def test_basic_functionality():
    """Testa funcionalidades bÃ¡sicas do OLABOT."""
    print("ğŸ§ª Testando Funcionalidades BÃ¡sicas...")
    
    try:
        from olasis.chatbot_v2 import OlaBot
        
        # Inicializar sem API key para teste de fallback
        bot = OlaBot(api_key=None, enable_prompt_engineering=True)
        
        # Teste de disponibilidade
        print(f"   âœ… Disponibilidade: {bot.is_available}")
        
        # Teste de resposta de fallback
        response = bot.ask("O que Ã© inteligÃªncia artificial?")
        assert len(response) > 50, "Resposta muito curta"
        print(f"   âœ… Resposta de fallback: {len(response)} caracteres")
        
        # Teste de estatÃ­sticas
        stats = bot.get_session_stats()
        assert "total_queries" in stats, "EstatÃ­sticas incompletas"
        print(f"   âœ… EstatÃ­sticas: {stats['total_queries']} consultas")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Erro: {e}")
        return False

def test_prompt_engineering():
    """Testa sistema de engenharia de prompt."""
    print("ğŸ§ª Testando Engenharia de Prompt...")
    
    try:
        from olasis.prompt_engineering import PromptBuilder, ResponseOptimizer
        
        # Teste do construtor de prompts
        builder = PromptBuilder()
        
        # Teste de detecÃ§Ã£o de contexto
        contexts = [
            ("Como buscar artigos sobre IA?", "search"),
            ("O que Ã© machine learning?", "concept"),
            ("Qual metodologia usar?", "methodology"),
            ("Estou comeÃ§ando minha pesquisa", "general")
        ]
        
        for message, expected in contexts:
            detected = builder.detect_context_type(message)
            print(f"   âœ… '{message[:30]}...' â†’ {detected}")
        
        # Teste de construÃ§Ã£o de prompt
        prompt = builder.build_contextual_prompt(
            user_message="Como fazer uma revisÃ£o sistemÃ¡tica?",
            context_type="methodology"
        )
        assert "OLABOT" in prompt, "Prompt nÃ£o contÃ©m identidade"
        assert "metodologia" in prompt.lower(), "Prompt nÃ£o contÃ©m contexto apropriado"
        print(f"   âœ… Prompt construÃ­do: {len(prompt)} caracteres")
        
        # Teste do otimizador de respostas
        optimizer = ResponseOptimizer()
        
        test_response = "**Isso** Ã© um *teste* com ### markdown"
        cleaned = optimizer.format_response(test_response)
        assert "**" not in cleaned, "Markdown nÃ£o removido"
        print(f"   âœ… Limpeza de markdown: '{test_response}' â†’ '{cleaned}'")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Erro: {e}")
        return False

def test_context_detection():
    """Testa detecÃ§Ã£o automÃ¡tica de contexto."""
    print("ğŸ§ª Testando DetecÃ§Ã£o de Contexto...")
    
    try:
        from olasis.prompt_engineering import PromptBuilder
        
        builder = PromptBuilder()
        
        test_cases = [
            # Busca
            ("Onde encontrar artigos sobre diabetes?", "search"),
            ("Como procurar estudos de sustentabilidade?", "search"),
            ("Preciso buscar literatura sobre AI", "search"),
            
            # Metodologia
            ("Como fazer anÃ¡lise qualitativa?", "methodology"),
            ("Qual procedimento usar para coleta de dados?", "methodology"),
            ("Que metodologia Ã© melhor para meu estudo?", "methodology"),
            
            # Conceito
            ("O que significa revisÃ£o sistemÃ¡tica?", "concept"),
            ("Defina inteligÃªncia artificial", "concept"),
            ("Explique o conceito de big data", "concept"),
            
            # Geral
            ("Estou comeÃ§ando meu doutorado", "general"),
            ("Preciso de ajuda com minha tese", "general")
        ]
        
        correct_detections = 0
        for message, expected in test_cases:
            detected = builder.detect_context_type(message)
            is_correct = detected == expected
            status = "âœ…" if is_correct else "âŒ"
            print(f"   {status} '{message[:40]}...' â†’ {detected} (esperado: {expected})")
            if is_correct:
                correct_detections += 1
        
        accuracy = (correct_detections / len(test_cases)) * 100
        print(f"   ğŸ“Š PrecisÃ£o da detecÃ§Ã£o: {accuracy:.1f}% ({correct_detections}/{len(test_cases)})")
        
        return accuracy > 80  # Pelo menos 80% de precisÃ£o
        
    except Exception as e:
        print(f"   âŒ Erro: {e}")
        return False

def test_response_quality():
    """Testa validaÃ§Ã£o de qualidade das respostas."""
    print("ğŸ§ª Testando ValidaÃ§Ã£o de Qualidade...")
    
    try:
        from olasis.prompt_engineering import ResponseOptimizer
        
        optimizer = ResponseOptimizer()
        
        test_responses = [
            # Resposta boa
            ("Esta Ã© uma resposta completa e Ãºtil sobre pesquisa cientÃ­fica. Recomendo consultar fontes confiÃ¡veis e usar mÃ©todos apropriados para sua investigaÃ§Ã£o.", True),
            
            # Resposta muito curta
            ("Ok", False),
            
            # Resposta com markdown
            ("**Esta** resposta tem *formataÃ§Ã£o* ### indevida", False),
            
            # Resposta muito longa (simulada)
            ("x" * 3000, False),
            
            # Resposta sem conteÃºdo portuguÃªs
            ("This is an English response without Portuguese content", False)
        ]
        
        passed_tests = 0
        for response, should_pass in test_responses:
            validation = optimizer.validate_response_quality(response)
            overall_quality = all(validation.values())
            
            status = "âœ…" if (overall_quality == should_pass) else "âŒ"
            print(f"   {status} Qualidade: {validation}")
            
            if overall_quality == should_pass:
                passed_tests += 1
        
        success_rate = (passed_tests / len(test_responses)) * 100
        print(f"   ğŸ“Š Taxa de sucesso: {success_rate:.1f}%")
        
        return success_rate >= 80
        
    except Exception as e:
        print(f"   âŒ Erro: {e}")
        return False

def test_integration():
    """Testa integraÃ§Ã£o com sistema existente."""
    print("ğŸ§ª Testando IntegraÃ§Ã£o...")
    
    try:
        # Teste de importaÃ§Ã£o compatÃ­vel
        from olasis import Chatbot, OlaBot
        
        # Testar que Chatbot ainda funciona (compatibilidade)
        old_bot = Chatbot(api_key=None)
        response = old_bot.ask("Teste")
        print(f"   âœ… Compatibilidade Chatbot: {len(response)} caracteres")
        
        # Testar novo OlaBot
        new_bot = OlaBot(api_key=None)
        response = new_bot.ask("Teste")
        print(f"   âœ… Novo OlaBot: {len(response)} caracteres")
        
        # Verificar que sÃ£o diferentes (novo tem mais funcionalidades)
        assert hasattr(new_bot, 'get_session_stats'), "OlaBot sem mÃ©todos avanÃ§ados"
        assert hasattr(new_bot, 'set_temperature'), "OlaBot sem configuraÃ§Ã£o"
        print(f"   âœ… Funcionalidades avanÃ§adas disponÃ­veis")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Erro: {e}")
        return False

def performance_test():
    """Teste de performance bÃ¡sico."""
    print("ğŸ§ª Testando Performance...")
    
    try:
        from olasis.chatbot_v2 import OlaBot
        from olasis.prompt_engineering import PromptBuilder
        
        bot = OlaBot(api_key=None, enable_prompt_engineering=True)
        builder = PromptBuilder()
        
        # Teste de mÃºltiplas consultas
        start_time = time.time()
        
        for i in range(10):
            message = f"Teste de performance {i}"
            context = builder.detect_context_type(message)
            response = bot.ask(message)
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"   âœ… 10 consultas em {duration:.2f} segundos")
        print(f"   âœ… MÃ©dia: {duration/10:.3f} segundos por consulta")
        
        # Verificar estatÃ­sticas
        stats = bot.get_session_stats()
        print(f"   âœ… Total de consultas: {stats['total_queries']}")
        print(f"   âœ… Taxa de sucesso: {stats['success_rate']:.1f}%")
        
        return duration < 10  # Menos de 10 segundos para 10 consultas
        
    except Exception as e:
        print(f"   âŒ Erro: {e}")
        return False

def main():
    """Executa todos os testes."""
    print("ğŸš€ INICIANDO TESTES DO OLABOT v2")
    print("=" * 50)
    
    tests = [
        ("Funcionalidades BÃ¡sicas", test_basic_functionality),
        ("Engenharia de Prompt", test_prompt_engineering),
        ("DetecÃ§Ã£o de Contexto", test_context_detection),
        ("Qualidade das Respostas", test_response_quality),
        ("IntegraÃ§Ã£o", test_integration),
        ("Performance", performance_test)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            result = test_func()
            results.append((test_name, result))
            status = "âœ… PASSOU" if result else "âŒ FALHOU"
            print(f"   Resultado: {status}")
        except Exception as e:
            results.append((test_name, False))
            print(f"   âŒ ERRO CRÃTICO: {e}")
    
    # RelatÃ³rio final
    print(f"\n{'='*50}")
    print("ğŸ“Š RELATÃ“RIO FINAL")
    print(f"{'='*50}")
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "âœ…" if result else "âŒ"
        print(f"   {status} {test_name}")
    
    print(f"\nğŸ¯ RESUMO: {passed}/{total} testes passaram ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("ğŸ‰ TODOS OS TESTES PASSARAM! Sistema pronto para uso.")
        return 0
    else:
        print("âš ï¸ Alguns testes falharam. Verifique os erros acima.")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
